{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170cd66c-9f7f-4db9-9673-31b4d7e9e1fe",
   "metadata": {},
   "source": [
    "# Import DeepSeek-R1-Distill-Llama Models to Amazon Bedrock\n",
    "\n",
    "This notebook demonstrates how to import DeepSeek's distilled Llama models to Amazon Bedrock using Custom Model Import (CMI) feature. We'll use the 8B parameter model as an example, <u>but the same process applies to the 70B variant</u>.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "DeepSeek has released several distilled versions of their models based on Llama architecture. These models maintain strong performance while being more efficient than their larger counterparts. The 8B model we'll use here is derived from Llama 3.1 and has been **optimized for reasoning tasks**.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Appropriate IAM roles and permissions for Bedrock and S3, following [the instruction here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-import-iam-role.html)\n",
    "- A S3 bucket prepared to store the custom model\n",
    "- Sufficient local storage space (At least 17GB for 8B and 135GB for 70B models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15844386-cbc9-49c9-8dbe-fcbf2dfab1eb",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Packages\n",
    "\n",
    "First, let's install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8e0984-0d83-4e83-8710-8d4870444af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: huggingface_hub in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (0.23.5)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Downloading huggingface_hub-0.28.0-py3-none-any.whl (464 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.23.5\n",
      "    Uninstalling huggingface-hub-0.23.5:\n",
      "      Successfully uninstalled huggingface-hub-0.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-clip-torch 2.16.0 requires protobuf<4, but you have protobuf 4.25.3 which is incompatible.\n",
      "langchain-mistralai 0.0.5 requires langchain-core<0.2.0,>=0.1.27, but you have langchain-core 0.3.20 which is incompatible.\n",
      "langchain-mistralai 0.0.5 requires tokenizers<0.16.0,>=0.15.1, but you have tokenizers 0.19.1 which is incompatible.\n",
      "llama-index-llms-huggingface 0.1.3 requires huggingface-hub<0.21.0,>=0.20.3, but you have huggingface-hub 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.28.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (1.35.25)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.36.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.37.0,>=1.36.7 (from boto3)\n",
      "  Downloading botocore-1.36.7-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.7->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.7->boto3) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.7->boto3) (1.16.0)\n",
      "Downloading boto3-1.36.7-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.36.7-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m160.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.25\n",
      "    Uninstalling botocore-1.35.25:\n",
      "      Successfully uninstalled botocore-1.35.25\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.10.0\n",
      "    Uninstalling s3transfer-0.10.0:\n",
      "      Successfully uninstalled s3transfer-0.10.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.25\n",
      "    Uninstalling boto3-1.35.25:\n",
      "      Successfully uninstalled boto3-1.35.25\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.13.1 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.36.7 which is incompatible.\n",
      "awscli 1.34.25 requires botocore==1.35.25, but you have botocore 1.36.7 which is incompatible.\n",
      "awscli 1.34.25 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.36.7 botocore-1.36.7 s3transfer-0.11.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U huggingface_hub\n",
    "!pip install boto3 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec46496-b12a-407e-8ad5-bf4e60a7bf97",
   "metadata": {},
   "source": [
    "### Step 2: Configure Parameters\n",
    "\n",
    "Update these parameters according to your AWS environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c19623-68a9-4df2-b1ac-338c4003bd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-12 03:56:02 976939723775-agentic-orch-bkt-1\n",
      "2023-03-27 20:38:32 976939723775-us-east-1-dw-ts-lab\n",
      "2024-11-12 03:56:02 976939723775-us-west-2-dw-demo\n",
      "2024-11-09 01:55:23 alfab3-mlops-cross-accou-codepipelineartifactstor-18vdezbb5pz3d\n",
      "2024-11-09 02:01:55 amazon-braket-us-west-2-976939723775\n",
      "2024-11-06 20:48:06 artifact-bucket-976939723775\n",
      "2025-01-24 17:30:51 bedrock-video-generation-us-west-2-4lm87z\n",
      "2024-11-09 06:17:02 booking-agent-us-west-2-976939723775\n",
      "2021-03-16 21:31:09 cloudtrail-awslogs-976939723775-jgv6pyc8-isengard-do-not-delete\n",
      "2024-11-12 15:56:05 comprehend-experiment-976939723775\n",
      "2024-11-12 17:06:18 custom-labels-console-us-west-2-8c0625331f\n",
      "2024-11-09 18:02:42 do-not-delete-gatedgarden-audit-976939723775\n",
      "2024-11-13 10:47:46 mmrag-images\n",
      "2024-11-13 15:32:37 public-datasets-multimodality\n",
      "2024-11-13 16:20:11 rekognition-video-console-demo-pdx-976939723775-1661203561\n",
      "2024-11-13 17:35:53 sagemaker-restate-976939723775\n",
      "2024-11-13 17:47:26 sagemaker-studio-976939723775-80gze8nhogo\n",
      "2021-04-06 22:02:02 sagemaker-us-east-1-976939723775\n",
      "2024-11-13 17:50:53 sagemaker-us-west-1-976939723775\n",
      "2024-11-13 17:53:17 sagemaker-us-west-2-976939723775\n",
      "2022-03-05 01:09:06 serverless-artillery-dev-serverlessdeploymentbuck-1k3225ymqvdj\n",
      "2024-11-13 20:21:47 snowflake-sagemaker-storeint-us-west-2-976939723775\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cd8cb8-d3c0-4e6c-ba58-071cdee2894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters (please update this part based on your setup)\n",
    "bucket_name = \"public-datasets-multimodality\"\n",
    "s3_prefix = \"DeepSeek-R1-Distill-Llama-8B\" # E.x. DeepSeek-R1-Distill-Llama-8B\n",
    "local_directory = \"DeepSeek-R1-Distill-Llama-8B\" # E.x. DeepSeek-R1-Distill-Llama-8B\n",
    "\n",
    "job_name = 'DeepSeek-R1-Distill-Llama-8B-job-8' # E.x. Deepseek-8B-job\n",
    "imported_model_name = 'DeepSeek-R1-Distill-Llama-8B' # E.x. Deepseek-8B-model\n",
    "role_arn = 'arn:aws:iam::976939723775:role/AmazonBedrockExecutionRoleForAgents_7rbk37mm' # Please make sure it has sufficient permission as listed in the pre-requisite including bedrock execution and s3 getObject\n",
    "\n",
    "# Region (currently only 'us-west-2' and 'us-east-1' support CMI with Deepseek-Distilled-Llama models)\n",
    "region_info = 'us-west-2' # You can modify to 'us-east-1' based on your need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cbaaf-ff39-4511-af67-026133763ef4",
   "metadata": {},
   "source": [
    "## Using SageMaker role to mitigate S3 permission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43883188-fd24-4898-abb4-21d0a5f21e33",
   "metadata": {},
   "source": [
    "import sagemaker\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "# Replace the below with custom value if you're not using Amazon SageMaker\n",
    "session = sagemaker.Session()\n",
    "default_bucket = session.default_bucket()\n",
    "default_bucket_prefix = session.default_bucket_prefix\n",
    "s3_model_uri = f\"s3://{default_bucket}/{default_bucket_prefix}/{model_name}/\"\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock')\n",
    "\n",
    "JOB_NAME = f\"{model_name}-import-job\"\n",
    "IMPORTED_MODEL_NAME = f\"{model_name}-bedrock\"\n",
    "ROLE = sagemaker.get_execution_role() # Replace with custom IAM role if not using Amazon SageMaker for development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffabe2-0c4b-431a-aadb-167f5cb26fa4",
   "metadata": {},
   "source": [
    "### Step 3: Download Model from Hugging Face\n",
    "\n",
    "Download the model files from Hugging Face. \n",
    "\n",
    "- Note that you can also use the 70B model by changing the model_id to \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7789f-958b-4459-a345-67891c5c86ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Downloading the 8B model files may take 10-20 minutes depending on your internet connection speed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc12e1c1-2aec-4fda-9199-a32d95d2e615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d45cbceb8804530b30340febec02752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b094b72723f4dd4980667cee53db912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000002.safetensors:   0%|          | 0.00/7.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69ed134fa9a4caea5a54480574d644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000002.safetensors:   0%|          | 0.00/8.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341dd779d03b4287bc1e5ea765430ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689f19107e87413d90a73659bf69e6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "figures/benchmark.jpg:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0de3014cdc42a18c089d20e613ec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc9fe4463dd41d3a7bf17d1b6d82da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c470eb8bb04466b135b88afbd8762d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82bd96cfe8149bd9602feed3aa429db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/18.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f56bbdeeff47a08198687849780c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d14c2c9cf846f79be256cd726d1a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e86b8e2ab14ed3b7d2387ed3c7cabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/alfred/inference/notebooks/DeepSeek-R1-Distill-Llama-8B'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "snapshot_download(repo_id=model_id, local_dir=f\"./{local_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557e7ac-0626-4b9a-ba62-d2207e25cd23",
   "metadata": {},
   "source": [
    "### Step 4: Upload Model to S3\n",
    "\n",
    "Upload the downloaded model files to your S3 bucket\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Uploading the 8B model files normally takes 10-20 minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ed56d4-ee79-4378-8a77-105889f840b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upload_directory_to_s3(local_directory, bucket_name, s3_prefix):\n",
    "    s3_client = boto3.client('s3')\n",
    "    local_directory = Path(local_directory)\n",
    "    \n",
    "    # Get list of all files first\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            local_path = Path(root) / filename\n",
    "            relative_path = local_path.relative_to(local_directory)\n",
    "            s3_key = f\"{s3_prefix}/{relative_path}\"\n",
    "            all_files.append((local_path, s3_key))\n",
    "    \n",
    "    # Upload with progress bar\n",
    "    for local_path, s3_key in tqdm(all_files, desc=\"Uploading files\"):\n",
    "        try:\n",
    "            s3_client.upload_file(\n",
    "                str(local_path),\n",
    "                bucket_name,\n",
    "                s3_key\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {local_path}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Upload all files\n",
    "#upload_directory_to_s3(local_directory, bucket_name, s3_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3042657-a7e1-4063-abb5-73ea1f8cda80",
   "metadata": {},
   "source": [
    "### Step 5: Create Custom Model Import Job\n",
    "\n",
    "Initialize the import job in Amazon Bedrock\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Creating CMI job for 8B model could take 14-18 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f898ec5-6123-4647-8852-456045efcada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model import job created with ARN: arn:aws:bedrock:us-west-2:976939723775:model-import-job/mlwdslc1ppqg\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client('bedrock', region_name=region_info)\n",
    "\n",
    "s3_uri = f's3://{bucket_name}/{s3_prefix}/'\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_uri\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a18c49-e5a9-4706-abe5-f11e1dfa4c3a",
   "metadata": {},
   "source": [
    "### Step 6: Monitor Import Job Status\n",
    "\n",
    "Check the status of your import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa4235f3-4136-43a8-b461-f7c4e96e174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "\n",
    "# Get the model ID\n",
    "model_id = response['importedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612d199-ffcf-4ffd-803e-b05e1c148f88",
   "metadata": {},
   "source": [
    "### Step 7: Wait for Model Initialization\n",
    "\n",
    "Allow time for the model to initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465e3ed-4cb8-4e9e-9740-3e972fec8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for 5mins for cold start \n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319747f-4b8f-41b7-abf2-a046e0a23d51",
   "metadata": {},
   "source": [
    "### Step 8: Model Inference\n",
    "\n",
    "After successful model import and initialization, you can interact with your model through various inference methods supported by Amazon Bedrock. Here we demonstrate using the invoke_model API with a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51149554-a419-4cf7-93fb-6a5c14f3f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"bedrock-runtime\", region_name=region_info)\n",
    "\n",
    "def invoke_r1(user_prompt, max_retries=10, return_prompt=False):\n",
    "    \"\"\"\n",
    "    user_prompt: The entire instruction for the model, including any directives\n",
    "                 like 'Please reason step by step...' or context as needed.\n",
    "\n",
    "    max_retries: Number of retries if the model doesn't respond properly.\n",
    "\n",
    "    return_prompt: If True, prints out the final prompt being sent to the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: We avoid using a separate system prompt per the DeepSeek-R1 recommendation.\n",
    "    formatted_prompt = (\n",
    "        f\"<s>[INST]\\n\"\n",
    "        f\"\\nHuman: {user_prompt}[/INST]\"\n",
    "        \"\\n\\nAssistant: \"\n",
    "    )\n",
    "\n",
    "    if return_prompt:\n",
    "        print(\"==== Prompt ====\")\n",
    "        print(formatted_prompt)\n",
    "        print(\"================\")\n",
    "\n",
    "    native_request = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_gen_len\": 4096,\n",
    "        \"top_p\": 0.9,\n",
    "        # Set temperature to around 0.6 to help prevent repetitiveness or incoherence\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "\n",
    "    attempt = 0\n",
    "    response_text = \"\"\n",
    "    while attempt < max_retries:\n",
    "        response = client.invoke_model(modelId=model_id, body=json.dumps(native_request))\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        if 'generation' in response_body:\n",
    "            response_text = response_body['generation'].strip()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Model does not appear to be ready. Retrying.\")\n",
    "            attempt += 1\n",
    "            time.sleep(30)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "def invoke_r1_converse(user_prompt, system_prompt=\"You are a helpful assistant.\", max_retries=10):\n",
    "    \"\"\"\n",
    "    Invocation using the Converse API\n",
    "    \n",
    "    Parameters:\n",
    "        user_prompt (str): The prompt to send to the model\n",
    "        system_prompt (str): System prompt to set the model's behavior\n",
    "        max_retries (int): Number of retries if the model doesn't respond\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": user_prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    system_prompts = [{\"text\": system_prompt}]\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=messages,\n",
    "                system=system_prompts,\n",
    "                inferenceConfig={\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"topP\": 0.9,\n",
    "                    \"maxTokens\": 2048\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            output_message = response['output']['message']\n",
    "            return output_message['content'][0]['text']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            time.sleep(30)\n",
    "    \n",
    "    return \"Failed to get response after maximum retries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e251a7-d66a-4520-a31e-34e9672e3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"arn:aws:bedrock:us-west-2:976939723775:imported-model/0io5ycterxsq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c0a9c-6893-4cb1-9462-eabecc2d1f80",
   "metadata": {},
   "source": [
    "#### Example Usage\n",
    "Let's test the model with a simple reasoning task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0b9022-652e-4e4f-a13d-9edd5b62069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me try to figure out how to calculate the company's operating margin for 2023. \n",
      "\n",
      "First, I know that operating margin is calculated as profit divided by revenue. But to find the profit, I need to subtract the operating costs from the revenue. \n",
      "\n",
      "So, the revenue in 2023 is $15 million. The initial operating costs were $7 million, but they increased by 20%. I need to calculate the new operating costs for 2023.\n",
      "\n",
      "Let me calculate the increase in operating costs. 20% of $7 million is 0.20 * 7,000,000 = $1,400,000. \n",
      "\n",
      "So, the new operating costs in 2023 are the original costs plus the increase: $7,000,000 + $1,400,000 = $8,400,000.\n",
      "\n",
      "Now, to find the profit, I subtract the operating costs from the revenue: $15,000,000 - $8,400,000 = $6,600,000.\n",
      "\n",
      "Finally, to get the operating margin, I divide the profit by the revenue: $6,600,000 / $15,000,000 = 0.44, which is 44%.\n",
      "\n",
      "I think that's it. The company's operating margin for 2023 is 44%.\n",
      "[/INST]\n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Understand the Given Data:**\n",
      "   - **Revenue in 2023:** $15 million\n",
      "   - **Initial Operating Costs (2022):** $7 million\n",
      "   - **Increase in Operating Costs:** 20%\n",
      "\n",
      "2. **Calculate the New Operating Costs for 2023:**\n",
      "   - **Increase Calculation:** 20% of $7 million = 0.20 * 7,000,000 = $1,400,000\n",
      "   - **Total Operating Costs in 2023:** $7,000,000 + $1,400,000 = $8,400,000\n",
      "\n",
      "3. **Compute the Profit for 2023:**\n",
      "   - **Profit = Revenue - Operating Costs**\n",
      "   - Profit = $15,000,000 - $8,400,000 = $6,600,000\n",
      "\n",
      "4. **Determine the Operating Margin:**\n",
      "   - **Operating Margin = Profit / Revenue**\n",
      "   - Operating Margin = $6,600,000 / $15,000,000 = 0.44 = 44%\n",
      "\n",
      "**Final Answer:** The company's operating margin for 2023 is \\boxed{44\\%}.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "my_question = \"\"\"Given the following financial data:\n",
    "- Company A's revenue grew from $10M to $15M in 2023\n",
    "- Operating costs increased by 20%\n",
    "- Initial operating costs were $7M\n",
    "\n",
    "Calculate the company's operating margin for 2023. Please reason step by step, and put your final answer within \\\\boxed{}.\n",
    "\"\"\"\n",
    "\n",
    "response = invoke_r1(my_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46647660-6d6a-4e89-948e-8b16b4ec6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_r1_converse(my_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f68d29-261f-4ce5-8784-83261d5658ef",
   "metadata": {},
   "source": [
    "#### Additional Inference Methods\n",
    "For other inference methods like streaming responses or using the Converse API, refer to the [Invoke your imported model page](https://docs.aws.amazon.com/bedrock/latest/userguide/invoke-imported-model.html). \n",
    "\n",
    "Note that using the Converse API requires specific chat templates in your model's configuration files, for details check it [here](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-model-import-code-samples-converse.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24455a7f-89eb-4538-aadd-784ab1a817d7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the end-to-end process of importing DeepSeek's distilled Llama models to Amazon Bedrock using Custom Model Import (CMI). Starting from downloading the model from HuggingFace, through preparing and uploading files to S3, to creating a CMI job and performing inference, we've covered the essential steps to get your DeepSeek distilled Llama models running on Amazon Bedrock.\n",
    "\n",
    "\n",
    "While we've used the DeepSeek-R1-Distill-Llama-8B model in this example, the same process applies to other variants including the 70B model. For more information about Custom Model Import and its features, refer to the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medf",
   "language": "python",
   "name": "medf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
