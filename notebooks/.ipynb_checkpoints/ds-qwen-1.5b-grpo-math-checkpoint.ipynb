{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacde5fa-85f3-4355-85a9-442b3a4aa680",
   "metadata": {},
   "source": [
    "# Fine tune deepseek qwen 1.5B with GRPO\n",
    "\n",
    "In this notebook we will demonstrate how to use the GRPOTrainer from huggingface to finetune Deepseek R1 - Distilled Qwen 1.5B, outlined in the bottom reference. I have also shared a notebook with the current packages needed to be able to run this notebook. I am new to this, so comments and corrections are welcome!\n",
    "\n",
    "This notebook used 3 different reward functions, and used a prompt from the Deepseek paper (https://arxiv.org/abs/2501.12948). My reward functions:\n",
    "\n",
    "* Formatting, so that there is a marked <think></think> section, a summary of the solution, then the final answer in \\boxed{}.\n",
    "* Accuracy of the result\n",
    "* The Levenshtein distance between the summarized solution and the actual solution from the dataset, called solution_quality.\n",
    "\n",
    "<b>Before training (100 evaluation problems):</b>\n",
    "\n",
    "on deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B with 3179 test samples\n",
    "\n",
    "* formatting 0.675\n",
    "* accuracy 0.475\n",
    "* solution_quality 0.2648481333776768\n",
    "\n",
    "\n",
    "<b>After training:</b>\n",
    "\n",
    "* formatting 0.6225\n",
    "* accuracy 0.5425\n",
    "* solution_quality 0.26916747222129206\n",
    "* \n",
    "an increase of over 10% accuracy! WOW!\n",
    "\n",
    "\n",
    "<b>Next steps:¶</b>\n",
    "\n",
    "I would of course like to get a properly done training and evaluation set, a cluster of GPUs, and see how far we can take this.\n",
    "Contact me if you want to team up!\n",
    "\n",
    "Disclaimers:\n",
    "The loss quoted when running the trainer is not what you would expect be if you are used to Supervised Fine Tuning. In the GRPOTrainer, the loss term multiplying the advantage is set to zero, however, don't be afraid, the model is learning! I have the printer callback inform you that there is in fact a gradient.\n",
    "This is a demonstration notebook. To be able to fit on this an run in a reasonable amount of time, I have selected a simpler and smaller dataset, shortened the output sequences, used the smallest possible model, and set the number of generations for use in training to 4. I have also used PEFT to decrease the size.\n",
    "\n",
    "\n",
    "<b>References</b>\n",
    "\n",
    "* https://www.kaggle.com/models/deepseek-ai/deepseek-r1\n",
    "* https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/557197\n",
    "* https://huggingface.co/docs/trl/main/en/grpo_trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0cd02-7f28-45eb-acf4-391209307b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --no-index -v --find-links=/kaggle/input/aimo-packages/offline_packages trl --pre\n",
    "!python -m pip install --no-index -v --find-links=/kaggle/input/aimo-packages/offline_packages levenshtein\n",
    "!python -m pip install --no-index -v --find-links=/kaggle/input/aimo-packages/offline_packages -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5609dc-e06e-47e2-94e1-f38cb44a420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "login(os.getenv('hf_api_wtoken'))\n",
    "wandb.login(key=os.getenv('wandb_api_token'))\n",
    "run = wandb.init(\n",
    "    project='Qwen-1.5B-grpo-imo', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3f5cd-40d6-4fae-9cff-df48f8198e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "import datetime\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig,\n",
    "    PrinterCallback,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Levenshtein import ratio as levenshtein_ratio\n",
    "transformers.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a327f83-08de-4da1-9309-dd7f994b11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    MAX_TRAIN = 1000\n",
    "    MAX_TOKENS = 4096\n",
    "    NUM_GENERATIONS = 4\n",
    "    USE_PEFT = True\n",
    "    BATCH_SIZE=1\n",
    "    MAX_STEPS = 80\n",
    "    \n",
    "    BETA = 0.04\n",
    "    LR = 1.e-5\n",
    "    \n",
    "    model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "    splitter = '<｜Assistant｜>'\n",
    "    \n",
    "    step_count=10\n",
    "    DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9da121-7a6f-43cb-bccd-719ed0076c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f891276-bd82-4a1e-974b-73bdb48dcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('~/data/imo/math_problems.parquet')\n",
    "df = df.reset_index().rename({'index': 'id'}, axis=1)\n",
    "df['answer'] = df['solution'].map(extract_boxed_text)\n",
    "\n",
    "def is_valid_answer(s):\n",
    "    try:\n",
    "        if float(s) == int(s):\n",
    "            i = int(s)\n",
    "            return 0<=i<1000\n",
    "        else:\n",
    "            return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "mask = df['answer'].map(is_valid_answer)\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03949e60-0658-4218-92e2-9a5bc0efa34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:CFG.MAX_TRAIN]\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457a00a-89b0-4d7c-98d2-46f025885bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a990b-cfed-43b3-a869-9eb18f311cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    question = sample['problem']\n",
    "    chat = [{\"role\": \"system\", \"content\": \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.  The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>\"},\n",
    "            {\"role\": \"user\", \"content\": question + ' Return final answer within \\\\boxed{}, after taking modulo 1000.'},]\n",
    "    sample['prompt'] = tokenizer.apply_chat_template(\n",
    "            conversation=chat,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6417d-2b7c-4062-8ccf-9a79e1024cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b7a24-0173-4387-8174-561e45c31766",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We would also want a reward function based on accuracy\n",
    "# split after </think>, then get the answer within bbox\n",
    "\n",
    "## We can also do a reward based on Similarity of \n",
    "\n",
    "import re\n",
    "\n",
    "def format_reward_func(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>.*?</think>.*?oxed{(.*?)}.*?$\"\n",
    "    matches = [re.match(pattern, content, re.DOTALL) for content in completions]\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "def accuracy_reward_func(completions, answer, **kwargs):\n",
    "    # Regular expression to capture content inside \\boxed{}\n",
    "    contents = [extract_boxed_text(completion) for completion in completions]\n",
    "    # Reward 1 if the content is the same as the ground truth, 0 otherwise\n",
    "    return [1.0 if c == str(gt) else 0.0 for c, gt in zip(contents, answer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56501f-6801-478c-a2e4-1aea834a2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_reward_func(completions, solution, **kwargs):\n",
    "    res = []\n",
    "    for completion, sol in zip(completions, solution):\n",
    "        if '</think>' in completion:\n",
    "            t = completion.split('</think>')[-1]\n",
    "            res.append(levenshtein_ratio(t, sol))\n",
    "        else:\n",
    "            res.append(0.0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f33f59-75aa-4cc1-97d2-c8b3a895c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = 'auto'\n",
    "if CFG.USE_PEFT:\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(CFG.model_name, \n",
    "                                                          device_map=device_map,\n",
    "                                                          quantization_config=bnb_config,\n",
    "                                                          trust_remote_code=True)\n",
    "else:\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(CFG.model_name, \n",
    "                                                          device_map=device_map,\n",
    "                                                          trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1693b40-4689-49fa-953d-13235f289e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name,trust_remote_code=True,padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311f364-3814-49ff-be5f-406be7068f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(create_prompt)#, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6da339-443a-4a87-9ae5-1fdafa1b4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model, text, max_tokens):\n",
    "    model_input = tokenizer(text, return_tensors='pt').to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tok = model.generate(**model_input, max_new_tokens=max_tokens, pad_token_id=tokenizer.pad_token_type_id)\n",
    "        outputs = []\n",
    "        for i in range(len(tok)):\n",
    "            res = tokenizer.decode(tok[i], skip_special_tokens=True)\n",
    "            output = res.split(CFG.splitter)[-1]\n",
    "            outputs.append(output)\n",
    "        return outputs[0] if len(outputs) == 1 else outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee0bb0-708b-45ec-81b9-7d06cf2c55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rewards(model, dataset, reward_functions: dict[str, callable], max_tokens: int, num_generations: int):\n",
    "    completions = []\n",
    "    other_info = []\n",
    "    for example in tqdm(dataset):\n",
    "        txt = example['prompt']\n",
    "        kw = {k: v for k, v in example.items() if k not in {'prompt', 'completion'}}\n",
    "        for _ in range(num_generations):\n",
    "            other_info.append(kw)\n",
    "            \n",
    "        completion = gen(model, [txt]*num_generations, max_tokens)\n",
    "        if isinstance(completion, str):\n",
    "            completions.append(completion)\n",
    "        else:\n",
    "            completions += completion\n",
    "        \n",
    "    kwargs = {k: [d[k] for d in other_info] for k in other_info[0].keys()}\n",
    "    res = {}\n",
    "    for nm, reward_func in reward_functions.items():\n",
    "        v = reward_func(completions=completions, **kwargs)\n",
    "        print(nm, np.mean(v))\n",
    "        res[nm] = np.mean(v)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e37a0-8c97-4522-8d31-cdd97e8d85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_functions = {'formatting': format_reward_func, 'accuracy': accuracy_reward_func, 'solution_quality': levenshtein_reward_func}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3a107-990c-4abe-a1b9-21e2df530bff",
   "metadata": {},
   "source": [
    "### Evaluate the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad12e27-7495-492b-b508-4ffc607f9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.DEBUG:\n",
    "    original_rewards = evaluate_rewards(model=original_model, dataset=dataset['test'], reward_functions=reward_functions, max_tokens=CFG.MAX_TOKENS, num_generations=CFG.NUM_GENERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa9e09f-0796-40a2-ad3c-958c6f0ea971",
   "metadata": {},
   "source": [
    "### Configure GRPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f05000-9041-4c4b-851b-ac434a990448",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtstr = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "output_directory=f\"./DEEPSEEK-GRPO-{dtstr}\"\n",
    "\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_directory,\n",
    "    \n",
    "    learning_rate=CFG.LR,\n",
    "    \n",
    "    per_device_train_batch_size=CFG.BATCH_SIZE,\n",
    "    \n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=CFG.MAX_STEPS,\n",
    "    \n",
    "    max_completion_length=CFG.MAX_TOKENS,  #8192\n",
    "    num_generations=CFG.NUM_GENERATIONS,\n",
    "    beta=CFG.BETA,\n",
    "    \n",
    "    logging_steps=CFG.step_count,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CFG.step_count,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=CFG.step_count,\n",
    "#     do_eval=True,\n",
    "    # gradient_checkpointing=True,  # Will crash the whole thing\n",
    "    report_to=\"wandb\", #\"none\"\n",
    "    overwrite_output_dir = 'True',\n",
    ")\n",
    "\n",
    "# Will typically use the AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef4a45f-9695-4855-9ade-c95fcf68277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.USE_PEFT:\n",
    "    peft_config = LoraConfig(\n",
    "        r=32, #Rank\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            'q_proj',\n",
    "            'k_proj',\n",
    "            'v_proj',\n",
    "            'dense'\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    trainer = GRPOTrainer(\n",
    "        model=original_model,\n",
    "        reward_funcs=list(reward_functions.values()),\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        peft_config=peft_config,\n",
    "        callbacks=[PrinterCallback()]\n",
    "    )\n",
    "else:\n",
    "    trainer = GRPOTrainer(\n",
    "        model=original_model,\n",
    "        reward_funcs=list(reward_functions.values()),\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        callbacks=[PrinterCallback()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6688e-ac99-4bec-bf50-4bace6d8451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd278a52-a5a8-4a46-b1c8-7542af3f8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.USE_PEFT:\n",
    "    print('Loading trained model')\n",
    "    CHKPT = CFG.MAX_STEPS\n",
    "    adapter_model_name = f'{output_directory}/checkpoint-{CHKPT}/'\n",
    "    new_model = PeftModel.from_pretrained(original_model, adapter_model_name)\n",
    "else:\n",
    "    new_model = original_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e77ff6-6269-47c2-b678-cf185344d69e",
   "metadata": {},
   "source": [
    "### Evaluate fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174f37d-c4a4-4f64-950d-e353c6cab5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = evaluate_rewards(model=new_model, dataset=dataset['test'], reward_functions=reward_functions, max_tokens=CFG.MAX_TOKENS, num_generations=CFG.NUM_GENERATIONS)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b1d0c-2bc1-4324-bf50-1d576e739e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
